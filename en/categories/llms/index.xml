<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>LLMs on MiaoMiaoYang's BLOG</title><link>https://miaomiaoyoung.github.io/en/categories/llms/</link><description>Recent content in LLMs on MiaoMiaoYang's BLOG</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>miaomiaoyangyxy@gmail.com (MiaoMiaoYang)</managingEditor><webMaster>miaomiaoyangyxy@gmail.com (MiaoMiaoYang)</webMaster><copyright>©2025, All Rights Reserved</copyright><lastBuildDate>Sun, 19 May 2024 15:01:38 +0800</lastBuildDate><atom:link href="https://miaomiaoyoung.github.io/en/categories/llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Public Datasets</title><link>https://miaomiaoyoung.github.io/en/posts/%E5%AD%A6%E4%B9%A0/papers/publicdata/</link><pubDate>Sun, 19 May 2024 15:01:38 +0800</pubDate><author>miaomiaoyangyxy@gmail.com (MiaoMiaoYang)</author><atom:modified>Sun, 19 May 2024 15:01:38 +0800</atom:modified><guid>https://miaomiaoyoung.github.io/en/posts/%E5%AD%A6%E4%B9%A0/papers/publicdata/</guid><description>ImageNet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train.tar https://image-net.org/challenges/LSVRC/2012/2012-downloads.php 1 mkdir train &amp;amp;&amp;amp; tar -xvf ILSVRC2012_img_train.tar -C train &amp;amp;&amp;amp; for x in `ls train/*tar`; do fn=train/`basename $x .tar`; mkdir $fn; tar -xvf $x -C $fn; rm -f $fn.tar; done 整理val到分类文件夹 https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh MIMIC-CXR https://physionet.org/content/mimic-cxr-jpg/2.1.0/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</description><dc:creator>MiaoMiaoYang</dc:creator><category>学习</category><category>work</category><category>paper</category><category>LLMs</category></item><item><title>Large Language Model</title><link>https://miaomiaoyoung.github.io/en/posts/%E5%AD%A6%E4%B9%A0/papers/llms/</link><pubDate>Fri, 19 May 2023 15:01:38 +0800</pubDate><author>miaomiaoyangyxy@gmail.com (MiaoMiaoYang)</author><atom:modified>Fri, 19 May 2023 15:01:38 +0800</atom:modified><guid>https://miaomiaoyoung.github.io/en/posts/%E5%AD%A6%E4%B9%A0/papers/llms/</guid><description>Base LLM Auto-regressive Decoding https://blog.csdn.net/qq_47564006/article/details/135750787 自回归解码 在自然语言处理(NLP)中，大型语言模型(LLM)如Transformer进行推理时，自回归解码是一种生成文本的方式。在</description><dc:creator>MiaoMiaoYang</dc:creator><category>学习</category><category>work</category><category>paper</category><category>LLMs</category></item></channel></rss>