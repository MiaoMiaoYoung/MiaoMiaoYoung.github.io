[{"content":"Breaking Bad 从我个人的朴素感情来说，我从内心希望老白死无葬身之地，妻离子散。因为他是毒贩。\n这部剧对人物的刻画还是非常成功的，剧情也非常的有趣，下面就说一说我对每个人物的看法。\n沃尔特 【老白】 我觉得老白是一个绝顶聪明，有工作能力，但是是一个极其懦弱的人。\n","description":"","id":2,"section":"posts","tags":["nichijou","绝命毒师","Breaking Bad"],"title":"绝命毒师观后感","uri":"https://miaomiaoyoung.github.io/en/posts/%E7%BB%9D%E5%91%BD%E6%AF%92%E5%B8%88%E8%A7%82%E5%90%8E%E6%84%9F/"},{"content":"DAY 0 种菜的想法其实很突然，可能是看到了无穷小亮的视频，可能是实验不顺利，反正就在一系列无法追溯缘由的因果之下，买了九种花果蔬菜的种子，买了盆，买了土。\n但是一直没有种，可能由于懒吧。\n2021年5月22日，上午到中午，被袁隆平院士逝世的消息所悲恸，与我而言，他们象征着共和国在那个年代所创下的辉煌，\n而我自诩为一名科学家（虽然不知道还能在科研的道路上继续前行多久），尚不知道有没有能力接过这沉重的历史接力棒，他们的存在让我感到莫名的安心，国家的脊梁就在那里，\n我们还可以有充足的时间来长大，来犯错，就好像一个孩子可以无忧无虑的尽情的在知识的海洋中遨游\n但是他们走了，我们就是脊梁\n也是为了纪念袁老吧，在晚上的时候我把这些花花中了下去，总共九种：\n 草莓、西红柿、紫苏、牵牛花、蒲公英、薄荷、向日葵、薰衣草、萝卜\n DAY 2 (05.24) 水萝卜 向日葵 DAY 3 (05.25) 水萝卜 向日葵 牵牛花 DAY 4 (05.26) 水萝卜 向日葵 牵牛花 西红柿 薄荷 DAY 5 (05.27) 水萝卜 向日葵 牵牛花 西红柿 薄荷 DAY 7 (05.29) 水萝卜 向日葵 牵牛花 西红柿 薄荷 紫苏 薰衣草 DAY 8 (05.30) 水萝卜 向日葵 牵牛花 西红柿 薄荷 紫苏 薰衣草 ","description":"","id":3,"section":"posts","tags":null,"title":"花花观察日记","uri":"https://miaomiaoyoung.github.io/en/posts/%E8%8A%B1%E8%8A%B1%E8%A7%82%E5%AF%9F%E6%97%A5%E8%AE%B0/"},{"content":"Math Support (KaTex) 几乎所有的Hugo添加Math支持都是如下四部：\n  Create a partial under /layouts/partials/math.html\n这个/layouts/partials/math.html一般在主题当中就有提供，不需要自己再另行创建\n  Within this partial reference the Auto-render Extension or host these scripts locally.\n同上，查看主题当中的这个math.html文件，里面添加KaTex支持，一般是如下语句：\n1 2 3  \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.13.3/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-ThssJ7YtjywV52Gj4JE/1SQEDoMEckXyhkFVwaf4nDSm5OBlXeedVYjuuUd0Yua+\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.13.3/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-Bi8OWqMXO1ta+a4EPkZv7bYGIes7C3krGSZoTGNTAnAn5eYQc7IIXrJ/7ck1drAi\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.13.3/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;   同样，一般不需要自己创建\n  Include the partial in your templates like so:\n1 2 3  {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }}   这一步需要自己加入，一般是在**主题中的/layouts/partials/**找到文章的模板，在文章的模板里加入上述语句。\n如此模板，在themes\\hugo-clarity\\layouts_default\\baseof.html中，加入上述语句：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  {{- $s := .Site.Params }} {{- $p := .Params }} \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ .Lang }}\u0026#34; data-figures=\u0026#34;{{ $p.figurePositionShow }}\u0026#34;{{ if .IsPage }} class=\u0026#34;page\u0026#34;{{ end }}{{ if .IsHome }} class=\u0026#34;home\u0026#34;{{ end }}{{ with $s.enforceLightMode }} data-mode=\u0026#34;lit\u0026#34;{{ end }}{{ with $s.enforceDarkMode }} data-mode=\u0026#34;dim\u0026#34;{{ end }}\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;head\u0026#34; . }} \u0026lt;!-- styles definition--\u0026gt; {{- $options := (dict \u0026#34;targetPath\u0026#34; \u0026#34;css/styles.css\u0026#34; \u0026#34;outputStyle\u0026#34; \u0026#34;compressed\u0026#34; \u0026#34;enableSourceMap\u0026#34; \u0026#34;true\u0026#34;) -}} {{ $mainSassFile := \u0026#34;sass/main.sass\u0026#34; }} {{- $styles := resources.Get $mainSassFile | resources.ExecuteAsTemplate $mainSassFile . | resources.ToCSS $options | resources.Fingerprint \u0026#34;sha512\u0026#34; }} {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} ......     To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration\nTo enable KaTex on a per page basis include the parameter math: true in content files\n  添加.pdf支持   hugo pdf支持：https://github.com/anvithks/hugo-embed-pdf-shortcode\n  分别将layouts\\shortcodes和static\\js\\pdf-js拷贝到主目录对应位置中\n  ","description":"Hugo Setting.","id":4,"section":"posts","tags":["Hugo","Blog"],"title":"Hugo","uri":"https://miaomiaoyoung.github.io/en/posts/hugo/"},{"content":"参考资料：\n https://zhuanlan.zhihu.com/p/178402798  ","description":"","id":5,"section":"posts","tags":["pytorch","python"],"title":"pytorch分布式训练","uri":"https://miaomiaoyoung.github.io/en/posts/pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"content":"编写Dockerfile FROM 定制的镜像都是基于FROM镜像的，所以需要找一个基础镜像\n1 2 3 4  ## 基础镜像 FROM pytorch/pytorch:latest ## 维护者信息 LABEL maintainer \u0026#34;MiaoMiaoYang\u0026#34;   RUN RUN 相当于在容器中执行命令，有以下两种方式\n  Shell格式\n1  RUN \u0026lt;命令行命令\u0026gt;     Exec格式\n1  RUN [\u0026#34;可执行文件\u0026#34;,\u0026#34;参数1\u0026#34;,\u0026#34;参数2\u0026#34;,...]     注意：Dockerfile的指令每执行一次，都会在docker上新建一层，如果无意义的层过多，会造成镜像膨胀过大。\n1 2 3 4 5 6 7 8 9 10 11 12 13  FROM centos RUN yum install wget RUN wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; RUN tar -xvf redis.tar.gz #### 可简化为以下格式 #### 使用 \\ 进行换行 #### 使用 \u0026amp;\u0026amp; 符号进行连接 FROM centos RUN yum install wget \\  \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\  \u0026amp;\u0026amp; tar -xvf redis.tar.gz   CMD指令 类似于RUN指令，但是RUN在docker build构建镜像时构建，CMD在docker run时运行\nCOPY指令 从上下文目录中复制文件或目录到容器里的指定路径\n1 2 3  ## [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;]：可选参数，用户改变复制到容器内文件的拥有者和属组。 COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;源路径1\u0026gt;... \u0026lt;目标路径\u0026gt; COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026#34;\u0026lt;源路径1\u0026gt;\u0026#34;,... \u0026#34;\u0026lt;目标路径\u0026gt;\u0026#34;]   ADD指令 与COPY使用格式一致，不同的是在拷贝tar压缩文件（gzip,bzip2,xz）时，会自动进行复制并解压到目标路径。但是当压缩文件很大的情况下，会导致镜像构建很慢，或者导致崩溃。\nENV指令 设置环境变量\nEXPOSE指令 声明端口，在运行时用于随机端口映射\n构建Docker镜像 在Dockerfile文件的存放目录下，执行构建动作\n1 2 3  docker build -t [image name]:[image label] . ## example docker build -t miaomiaoyang/pytorch:test .   最后的.表示本次执行的上下文路径\n启动Docker容器 1  nvidia-docker run -itd --name nnunet --shm-size 6G -p 8000:80 -p 10022:22 -p 5000:5000 -v /home/MiaoMiaoYang:/MiaoMiaoYang miaomiaoyang/pytorch:test /bin/bash   Pytorch 示例   Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  FROM pytorch/pytorch:latest ## 维护者信息 LABEL maintainer \u0026#34;MiaoMiaoYang\u0026#34; ## 换源，将sources.list/requirements.txt放在 Dockerfile同目录下 ADD sources.list /etc/apt/ ADD requirements.txt /installer/ RUN apt-get update \\  \u0026amp;\u0026amp; apt-get upgrade --assume-yes \\  \u0026amp;\u0026amp; apt-get install vim --assume-yes \\  ## 添加SSHD服务 \u0026amp;\u0026amp; apt-get install openssh-server --assume-yes \\  \u0026amp;\u0026amp; apt-get install openssh-client --assume-yes \\  \u0026amp;\u0026amp; echo \u0026#39;root:123456\u0026#39;|chpasswd \\  \u0026amp;\u0026amp; mkdir -p /var/run/sshd \\  \u0026amp;\u0026amp; sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/g\u0026#39; /etc/ssh/sshd_config \\  ## pip 添加库 \u0026amp;\u0026amp; pip config set global.index-url https://pypi.douban.com/simple/ \\  \u0026amp;\u0026amp; pip install --no-cache-dir -r /installer/requirements.txt ## 开放端口22，镜像启动时启动SSHD服务 EXPOSE 22 CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;]     requirement.txt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  dicom2nifti==2.2.12 imageio==2.9.0 matplotlib==3.3.4 MedPy==0.4.0 natsort==7.1.1 packaging==20.9 pandas==1.2.3 Pillow==8.0.0 pydicom==2.1.2 pyOpenSSL==19.1.0 PyWavelets==1.1.1 PyYAML==5.3.1 scikit-image==0.18.1 scikit-learn==0.24.1 scipy==1.6.1 SimpleITK==2.0.2 six==1.14.0 sklearn==0.0 tqdm==4.46.0 traceback2==1.4.0     ","description":"","id":6,"section":"posts","tags":["linux","ubuntu","docker"],"title":"Dockerfile","uri":"https://miaomiaoyoung.github.io/en/posts/dockerfile/"},{"content":"参考资料\n https://blog.csdn.net/bigbennyguo/article/details/87956434 https://zhuanlan.zhihu.com/p/37022051  创建TensorBoardX 1 2 3 4  from tensorboardX import SummaryWriter writer = SummaryWriter(\u0026#39;runs/exp\u0026#39;) writer = SummaryWriter() writer = SummaryWriter(comment=\u0026#39;resnet\u0026#39;)   三种方式初始化TensorBoardX，创建SummaryWriter\n  提供路径，使用该路径保存日志\n  无参数，默认将使用runs/日期时间路径保存日志\n  提供一个comment参数，将使用runs/日期时间-comment路径来保存日志\n  查看数据 在命令行中使用如下命令，启动浏览器可视化数据：\n1  tensorboard --logdir=\u0026lt;log_dir\u0026gt;   其中的 \u0026lt;log_dir\u0026gt; 既可以是单个 run 的路径；也可以是多个 run 的父目录，如 runs/ 下面可能会有很多的子文件夹，每个文件夹都代表了一次实验，我们令 \u0026ndash;logdir=runs/ 就可以在 tensorboard 可视化界面中方便地横向比较 runs/ 下不同次实验所得数据的差异。\n1 2 3 4 5 6 7 8 9 10  from tensorboardX import SummaryWriter writer = SummaryWriter(\u0026#39;runs/scalar_example\u0026#39;) for i in range(10): writer.add_scalar(\u0026#39;quadratic\u0026#39;, i**2, global_step=i) writer.add_scalar(\u0026#39;exponential\u0026#39;, 2**i, global_step=i) writer = SummaryWriter(\u0026#39;runs/another_scalar_example\u0026#39;) for i in range(10): writer.add_scalar(\u0026#39;quadratic\u0026#39;, i**3, global_step=i) writer.add_scalar(\u0026#39;exponential\u0026#39;, 3**i, global_step=i)   1  tensorboard --logdir=runs   可以看到，runs下文件夹所有的相同指标的都被放到了同一张图表下进行展示，以方便横向对比\n指定端口 在启动tensorboard时添加端口号：\n1  tensorboard --logdir=yourpath --port=6005   服务器中的docker启动 1  tensorboard --logdir=/MiaoMiaoYang/cifar-10/runs/ --port=5000 --bind_all    将Docker中的端口映射出去 logdir中使用绝对路径 使用**\u0026ndash;bind_all**，不然会出现拒绝访问  记录数据 数字 (scalar) 1  add_scalar(tag, scalar_value, global_step=None, walltime=None)    tag: 数据名称 scalar_value: 数字常量 (float)，可以理解为纵轴如果是pytorch tensor，需要调用.item()获取其数值 global_step: 训练的step，可以理解为横轴 walltime: 记录放生的时间，default: time.time()  Example:\n1 2 3 4  writer = SummaryWriter() for i in range(10): writer.add_scalar(\u0026#39;quadratic\u0026#39;, i**3, global_step=i) writer.add_scalar(\u0026#39;exponential\u0026#39;, 3**i, global_step=i)   图片 (image) 1  add_image(tag, img_tensor, global_step=None, walltime=None, dataformats=\u0026#39;CHW\u0026#39;)    tag: 数据名称 img_tensor: 图像数据，支持numpy或torch tensor global_step: 训练的step，可以理解为横轴 walltime: 记录发生的时间，默认为time.time() dataformats: 图像数据的格式，default: \u0026ldquo;CHW\u0026rdquo;  1 2 3 4 5 6 7 8 9 10 11 12 13 14  import torchvision import numpy as np dataset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;) from tensorboardX import SummaryWriter writer = SummaryWriter(\u0026#39;runs/image_example\u0026#39;) for index, (image,label) in enumerate(dataset): ## image shape: (32,32,3) writer.add_image(\u0026#39;cifar-10\u0026#39;,np.asarray(image),global_step=index,dataformats=\u0026#39;HWC\u0026#39;) print(\u0026#39;label:{} index:{}\u0026#39;.format(label,index)) if index \u0026gt; 10: break   可以在image栏中看到随着step变化的图像变化\n add_image一次只能添加一张图片，可以使用torchvision中的make_grid方法将多张图片拼接成一张图片，在使用add_image进行展示。或者使用add_images。\n 直方图 (histogram) 1  add_histogram(tag, values, global_step=None, bins=\u0026#39;tensorflow\u0026#39;, walltime=None, max_bins=None)    tag: 数据名称 values: 用来构建直方图的数据 global_step: 训练的step，可以理解为横轴 bins: 决定分桶的方式，有：tensorflow, auto, fd等参数，详见 walltime: 记录发生的时间，默认为time.time() max_bins: 最大分桶数  1 2 3 4 5 6 7  from tensorboardX import SummaryWriter import numpy as np writer = SummaryWriter(\u0026#39;runs/embedding_example\u0026#39;) writer.add_histogram(\u0026#39;normal_centered\u0026#39;, np.random.normal(0, 1, 1000), global_step=1) writer.add_histogram(\u0026#39;normal_centered\u0026#39;, np.random.normal(0, 2, 1000), global_step=50) writer.add_histogram(\u0026#39;normal_centered\u0026#39;, np.random.normal(0, 3, 1000), global_step=100)   可以看到生成了两种图像可以看到数据的分布，一个是DISTRIBUTIONS，一个是HISTOGRAMS。\n在表达直方图时，可以选择两种模式，一种是OFFSET，会展示每一个梯度下他的直方图，如上图。另一种是OVERLAY，会展示叠加在一起的直方图，如下图。\n而DISTRIBUTION以另一种方式进行数据的展示，横轴是STEP，纵轴的五个点分别表示着数值的 [max, 93％, 84％, 69％, 50％, 31％, 16％, 7％, min]。\n这些百分位数也可以看作标准偏差的正态分布：[max, μ+1.5σ, μ+σ, μ+0.5σ, μ, μ-0.5σ, μ-σ, μ-1.5σ, min]，使得从内侧读到外侧的着色区域分别具有宽度[σ，2σ，3σ]。\n向量 (embedding) 1  add_embedding(mat, metadata=None, label_img=None, global_step=None, tag=\u0026#39;default\u0026#39;, metadata_header=None)    mat: 矩阵，识别torch.tensor / numpy.array。每行代表特征空间的一个数据点 metadata (optional): 一个一维列表，表示mat每行数据的label，个数应与mat行数相同。识别torch.tensor / numpy.array label_img (optional): 一个形如 NxCxHxW 的张量，对应 mat 每一行数据显示出的图像，N 应和 mat 行数相同 global_step: 训练的step tag: 数据名称  1 2 3 4 5 6 7 8 9 10 11 12  from tensorboardX import SummaryWriter import torchvision writer = SummaryWriter(\u0026#39;runs/embedding_example\u0026#39;) mnist = torchvision.datasets.MNIST(\u0026#39;mnist\u0026#39;, download=True) writer.add_embedding( mnist.data.reshape((-1, 28 * 28))[:100,:], metadata=mnist.targets[:100], label_img = mnist.data[:100,:,:].reshape((-1, 1, 28, 28)).float() / 255, global_step=0 )   选择PROJECTOR进行查看向量空间。可以选择2D/3D空间进行可视化。可视化的方法可以是t-SNE/PCA或其他。\n","description":"","id":7,"section":"posts","tags":["python","pycharm","torch","tensorboardx"],"title":"TensorboardX","uri":"https://miaomiaoyoung.github.io/en/posts/tensorboardx/"},{"content":"Trick 引用子图片 [https://wenda.latexstudio.net/q-1405.html]\n使用subfig宏包的办法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\documentclass[journal]{IEEEtran} \\usepackage{graphicx} \\usepackage{subfig} \\captionsetup[subfigure]{labelformat=simple} \\renewcommand\\thesubfigure{(\\alph{subfigure})} \\usepackage{hyperref} \\newcommand{\\subfigureautorefname}{Fig.} % the name is sub-figure-auto-ref-name  \\usepackage{mwe} % for use of sample images \\begin{document} \\begin{figure}[!t] \\centering \\subfloat[]{\\includegraphics[width=2.5in]{example-image-a}\\label{A}} \\\\ \\subfloat[]{\\includegraphics[width=2.5in]{example-image-b}\\label{B}} \\caption{LETTER } \\label{LETTER} \\end{figure} The letter A is shown in \\autoref{A} \\end{document}   使用subcaption宏包：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \\documentclass[journal]{IEEEtran} \\usepackage{graphicx} % ref: https://tex.stackexchange.com/a/512010 \\usepackage{subcaption} \\renewcommand\\thesubfigure{(\\alph{subfigure})} \\captionsetup[sub]{labelformat=simple} \\usepackage{hyperref} \\renewcommand{\\figureautorefname}{Fig.} \\usepackage{mwe} % for use of sample images \\begin{document} \\begin{figure}[!t] \\centering \\subfloat[]{\\includegraphics[width=2.5in]{example-image-a}\\label{A}} \\\\ \\subfloat[]{\\includegraphics[width=2.5in]{example-image-b}\\label{B}} \\caption{LETTER } \\label{LETTER} \\end{figure} The letter A is shown in \\autoref{A} \\end{document}   ","description":"","id":8,"section":"posts","tags":["latex"],"title":"Latex","uri":"https://miaomiaoyoung.github.io/en/posts/latex/"},{"content":"创建AWS账户 创建组 进入控制台：https://console.aws.amazon.com/\n创建组\n设置组名\n设置策略：s3-readonly和审核\n创建用户 设置用户名，访问权限\n将用户添加到组\n之后一路next，直到获得访问密钥\n安装CLI https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html\nhttps://awscli.amazonaws.com/AWSCLIV2.msi\n配置CLI 将前面的访问ID和私有访问密钥\naws s3命令下载数据集 1  aws s3 cp \u0026#34;s3://ml-inat-competition-datasets/2021/train.tar.gz\u0026#34; ./ --recursive   加速 https://blog.csdn.net/xuanwu_yan/article/details/79160034\n修改配置文件vim ~/.aws/config，末尾加入\n[default] output = json s3 = max_concurrent_requests =500 max_queue_size = 10001 multipart_threshold = 500MB ","description":"","id":9,"section":"posts","tags":["aws下载"],"title":"AWS s3 下载","uri":"https://miaomiaoyoung.github.io/en/posts/aws/"},{"content":"Subjective Logic学习笔记 ","description":"","id":10,"section":"posts","tags":null,"title":"Subjective Logic学习笔记","uri":"https://miaomiaoyoung.github.io/en/posts/subjectivelogic/"},{"content":"键盘说明书   #the-canvas { border: 1px solid black; direction: ltr; width: 100%; height: auto; } #paginator{ text-align: center; margin-bottom: 10px; }  Previous Next \u0026nbsp; \u0026nbsp; Page:  /     window.onload = function() { var url = \"https:\\/\\/miaomiaoyoung.github.io\\/\" + ''; var hidePaginator = \"\"; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = \"https:\\/\\/miaomiaoyoung.github.io\\/\" + '/js/pdf-js/build/pdf.worker.js'; var pdfDoc = null, pageNum = 1, pageRendering = false, pageNumPending = null, scale = 3, canvas = document.getElementById('the-canvas'), ctx = canvas.getContext('2d'); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas.height = viewport.height; canvas.width = viewport.width; var renderContext = { canvasContext: ctx, viewport: viewport }; var renderTask = page.render(renderContext); renderTask.promise.then(function() { pageRendering = false; if (pageNumPending !== null) { renderPage(pageNumPending); pageNumPending = null; } }); }); document.getElementById('page_num').textContent = num; } function queueRenderPage(num) { if (pageRendering) { pageNumPending = num; } else { renderPage(num); } } if (hidePaginator) { document.getElementById(\"paginator\").style.display = 'none'; } function onPrevPage() { if (pageNum = pdfDoc.numPages) { return; } pageNum++; queueRenderPage(pageNum); } document.getElementById('next').addEventListener('click', onNextPage); pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) { pdfDoc = pdfDoc_; document.getElementById('page_count').textContent = pdfDoc.numPages; renderPage(pageNum); }); }  ","description":"","id":11,"section":"posts","tags":["nichijou"],"title":"nichijou","uri":"https://miaomiaoyoung.github.io/en/posts/nichijou/"},{"content":"常用的tikz作图  常用的tikz作图  经过多个点的光滑曲线 神经网络   问题  node节点中的文字换行    经过多个点的光滑曲线 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\documentclass{article} \\usepackage{tikz} %% 使用tikz库 \\usetikzlibrary{through,hobby} \\begin{document} \\begin{tikzpicture}[use Hobby shortcut] \\draw[help lines] (0,0) grid (5,5); %% 定义坐标点  \\coordinate[label=left:$A_1$] (A1) at (0,0); \\coordinate[label=left:$A_2$] (A2) at (1,2); \\coordinate[label=left:$A_3$] (A3) at (2,1); \\coordinate[label=left:$A_4$] (A4) at (4,3); \\coordinate[label=left:$A_5$] (A5) at (5,5); %% 绘制曲线  \\draw[line width=.8pt] (A1) .. (A2) .. (A3) .. (A4) .. (A5); \\end{tikzpicture} \\end{document}   神经网络 Tikz三方库：\n https://github.com/HarisIqbal88/PlotNeuralNet http://www.giantpandacv.com/%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB/%E5%BF%AB%E9%80%9F%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8tikz%E7%BB%98%E5%88%B6CNN%E7%A4%BA%E6%84%8F%E5%9B%BE/  发现一个好玩的东西，在使用PlotNeuralNet的同时开着VScode，将.tex生成的文件作为VScode工作文件夹。\n这样在PlotNeuralNet生成.tex的同时，Latex Workshop检测到.tex文件变动，就会自己生成相应的pdf\n问题 node节点中的文字换行 加入align=center选项，使用\\进行换行\n1  \\node at(17,1.5,0) [align=center]{interpolated \\\\ slice};   ","description":"","id":12,"section":"posts","tags":["作图","tikz","latex"],"title":"Tikz收藏夹","uri":"https://miaomiaoyoung.github.io/en/posts/tikz%E6%94%B6%E8%97%8F%E5%A4%B9/"},{"content":"参考资料：\n  https://zhuanlan.zhihu.com/p/127155579\n  https://www.bu.edu/math/files/2013/08/tikzpgfmanual.pdf\n  https://www.latex4technics.com/\n  https://texample.net/tikz/resources/\n  配置环境\n 基础环境 Vscode中输出SVG矢量图像    Tikz语法\n 基本语法 绘制直线  基本命令 变成圆角 {[rounded corners]} 封闭图形 {cycle}   绘制基本图形  矩形 {rectangle} 圆 {circle} 椭圆 {ellipse} 圆弧 {arc} 曲线 {..} 抛物线 {parabola} 网格 {grid}   选项[option]  箭头 线型、线宽 变形  平移 缩放 旋转   颜色和填充   样式  全局样式 局部样式   节点 (node)  确定节点的样式 利用节点画流程图 子节点 插入外部图片   绘制函数    配置环境 基础环境 1 2 3 4 5 6 7 8 9 10  \\documentclass{article} \\usepackage{tikz} \\begin{document} \\begin{tikzpicture} ... \\end{tikzpicture} \\end{document}    测试，xlatex和pdflatex均可通过\n Tikz提供了 \\tikz 和 \\tikzpicture 环境两种。可以任意选择。\nVscode中输出SVG矢量图像  https://github.com/James-Yu/LaTeX-Workshop/issues/1762 https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ#how-to-pass--shell-escape-to-latexmk https://github.com/James-Yu/LaTeX-Workshop/issues/464  平台：VScode + Latex Workshop\n  在vscode插件Latex Workshop中设置latexmk编译器参数-shell-escape。\n第二个链接中，How to pass -shell-escape to latexmk 给出了三种解决办法，这里使用的第一种，直接加-shell-escape参数\n1 2 3 4 5 6 7 8 9 10 11  \u0026#34;name\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;latexmk\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-shell-escape\u0026#34;, \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, //\u0026#34;-pdf\u0026#34;, \u0026#34;-outdir=%OUTDIR%\u0026#34;, \u0026#34;%DOC%\u0026#34; ]     使用dvisvgm生成svg\n这里使用的是vscode中latex workshop里面的批指令\n在设置settings.json -\u0026gt; \u0026ldquo;latex-workshop.latex.tools\u0026rdquo; 中添加dvisvgm命令\n1 2 3 4 5 6 7  { \u0026#34;name\u0026#34;: \u0026#34;dvisvgm\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;dvisvgm\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;%DOCFILE%\u0026#34; ] },   在settings.json -\u0026gt; \u0026ldquo;latex-workshop.latex.recipes\u0026rdquo; 中添加recipes：\n1 2 3 4 5 6 7  { \u0026#34;name\u0026#34;: \u0026#34;svg\u0026#34;, \u0026#34;tools\u0026#34;: [ \u0026#34;latexmk\u0026#34;, \u0026#34;dvisvgm\u0026#34; ] }     latex模板\n1 2 3 4 5 6  \\documentclass[tikz]{standalone} \\begin{document} \\begin{tikzpicture} ... \\end{tikzpicture} \\end{document}     Tikz语法 基本语法 1  \\draw[option] ...   \\draw 为绘图命令；后面的\u0026hellip;为操作；[option]为选项部分，可不填。\n绘制直线 基本命令 在 \\draw 命令后，输入点的坐标，并使用 \u0026ndash; 连接起来即可，\n1 2 3  \\begin{tikzpicture} \\draw (1,3)--(2,2)--(4,5); \\end{tikzpicture}   变成圆角 {[rounded corners]} 1 2 3  \\begin{tikzpicture} \\draw[rounded corners] (1,3)--(2,2)--(4,5); \\end{tikzpicture}   封闭图形 {cycle} 1 2 3  \\begin{tikzpicture} \\draw[rounded corners] (1,3)--(2,2)--(4,5)--cycle; \\end{tikzpicture}   伪封闭图形：最后的连接处是不会执行圆角指令的，因为识别不出来是连接在一起的：\n绘制基本图形 矩形 {rectangle}  参数：一对对角顶点\n 1 2 3 4 5 6 7 8 9  %% 基本命令 \\begin{tikzpicture} \\draw (0,0) rectangle (4,2); \\end{tikzpicture} %% 圆角 \\begin{tikzpicture} \\draw[rounded corners] (0,0) rectangle (4,2); \\end{tikzpicture}   圆 {circle}  参数：圆心坐标；半径（小括号括起来）\n 1 2 3 4  %% 圆心在(1,1) 半径为1的圆 \\begin{tikzpicture} \\draw (1,1) circle (1); \\end{tikzpicture}   椭圆 {ellipse}  参数：重心；长轴长，短轴长（小括号括起来，使用and间隔）\n 1 2 3 4  %% 重心在(1,1) 长轴长为2，短轴长为1的椭圆 \\begin{tikzpicture} \\draw (1,1) ellipse (2 and 1); \\end{tikzpicture}   圆弧 {arc}  参数：重心（中心）; 起始角度：终止角度：圆心（长轴长 and 短轴长）\n 1 2 3 4 5 6  \\begin{tikzpicture} %% 绘制了圆心为(1,1)，半径为1，从0°到270°的圆弧  \\draw (1 ,1) arc (0:270:1); %% 绘制了重心为(6,1)，长轴2，短轴1，从0°到180°的椭圆弧  \\draw (6 ,1) arc (0:180:2 and 1); \\end{tikzpicture}   曲线  参数：控制点\n 和直线的画法基本一致，需要将直线的\u0026ndash;换为..\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \\begin{tikzpicture} %% 带一个控制点的贝塞尔曲线  \\filldraw [gray] (0,0) circle (.1) (1,1) circle (.1) (2,0) circle (.1); \\draw (0,0) .. controls (1,1) .. (2,0); %% 带两个控制点的贝塞尔曲线  \\filldraw [blue] (3,0) circle (.1) (4,1) circle (.1) (5,1) circle (.1) (5,0) circle (.1); \\draw (3,0) .. controls (4,1) and (5,1) .. (5,0); %% 两端贝塞尔样条链接  \\filldraw [red] (6,0) circle (.1) (7,1) circle (.1) (8,1) circle (.1) (8,0) circle (.1) (9,2) circle (.1) (10,0) circle (.1); \\draw (6,0) .. controls (7,1) and (8,1) .. (8,0) .. controls (9,2) .. (10,0); \\end{tikzpicture}   抛物线 {parabola}  参数：起点；终点； [顶点：bend]\n 1 2 3 4 5 6 7 8 9 10 11 12  \\begin{tikzpicture} %% 起点为(5,1) 终点为(7,2)的抛物线  \\filldraw [gray] (5,1) circle (.1) (7,2) circle (.1); \\draw (5,1) parabola (7,2); %% 起点为(10,1) 终点为(12,2) 顶点为(11,0) 的抛物线  \\filldraw [blue] (10,1) circle (.1) (11,0) circle (.1) (12,2) circle (.1); \\draw (10,1) parabola bend (11,0) (12,2); \\end{tikzpicture}   网格 {grid}  参数：起始点，步长\n 1 2 3 4 5 6 7 8 9 10  \\begin{tikzpicture} %% 左上：步长为20pt的网格  \\draw [step=20pt] (0,0) grid (3,2); %% 右上：步长为20pt,灰线为0.2pt（help lines）的网格  \\draw [help lines,step=20pt] (4,0) grid (7,2); %% 左下  \\draw [thick,step=20pt] (0,-3) grid (3,-1); %% 右下  \\draw [very thin,step=20pt] (4,-3) grid (7,-1); \\end{tikzpicture}   选项[option] 箭头 1 2 3 4 5 6 7  \\begin{tikzpicture} \\draw [|\u0026lt;-\u0026gt;|] (0,4)--(9,4); \\draw [\u0026gt;-\u0026gt;\u0026gt;] (0,3)--(9,3); \\draw [\u0026lt;-\u0026gt;] (0,2)--(9,2); \\draw [\u0026lt;-] (0,1)--(9,1); \\draw [-\u0026gt;] (0,0)--(9,0); \\end{tikzpicture}   线型、线宽 1 2 3 4 5 6 7 8 9  \\begin{tikzpicture} \\draw [loosely dashed] (0,6)--(9,6); %较疏的线状虚线  \\draw [densely dashed] (0,5)--(9,5); %较密的线状虚线  \\draw [dashed] (0,4)--(9,4); %线状虚线  \\draw [loosely dotted] (0,3)--(9,3); %较疏的点状虚线  \\draw [densely dotted] (0,2)--(9,2); %较密的点状虚线  \\draw [dotted] (0,1)--(9,1); %点状虚线  \\draw [line width =2pt] (0,0)--(9,0); %加粗实线 \\end{tikzpicture}   变形 平移 首先需要定义一个基础图形，然后所有的移动都是相对这个基本图形来的\n1 2 3 4 5 6 7 8  \\begin{tikzpicture} %% 首先定义一个基础图形  \\draw (0,0) rectangle (1,1); %% 平移shift，根据基本图形向右平移2个单位  \\draw[shift={(2,0)},color=red] (0,0) rectangle (1,1); %% 沿y轴平移，根据基本图形向下平移，默认单位是pt；沿x轴平移同理  \\draw[yshift=-30, color=blue] (0,0) rectangle (1,1); \\end{tikzpicture}   缩放 缩放的中心是按照起始点进行缩放\n1 2 3 4 5 6 7 8 9  \\begin{tikzpicture} \\draw (0,0) rectangle (2,2); \\draw (0,0) circle (.5); %% 整体缩放，是按照起始点进行缩放  \\draw [scale =.5, color = blue] (0,0) rectangle (2,2); \\draw [scale =1.5, color = red] (0,0) circle (.5); %% 单个方向进行缩放  \\draw [xscale =.5, color = green] (0,0) circle (.5); \\end{tikzpicture}   旋转 旋转的数值是角度，正数表示逆时针选咋混的相应角度。旋转中心是起始点\n1 2 3 4 5  \\begin{tikzpicture} \\draw (0,0) rectangle (2,2); %% 逆时针旋转45°  \\draw[rotate =45, color = red] (0,0) rectangle (2,2); \\end{tikzpicture}   指定点进行旋转\n1 2 3 4 5  \\begin{tikzpicture} \\draw (0,0) rectangle (1,2); %% 将整个图形以(1,2)点为中心进行逆时针旋转30°  \\draw[rotate around ={30:(1 ,2)}, color = red] (0,0) rectangle (1,2); \\end{tikzpicture}   颜色和填充 使用\\filldraw命令可以填充图形，在选项中指定线条和填充的颜色\n1 2 3 4 5 6  \\begin{tikzpicture} %% 画一个中心点为(0,0)半径为1的圆  %% 线条颜色是蓝色，透明度是80%  %% 填充颜色是红色，透明度是20%  \\filldraw[draw=blue!80,fill=red!20] (0,0) circle (1); \\end{tikzpicture}   样式 全局样式 在\\begin{document}之前使用\\tikzset指定样式\n1 2 3 4 5 6 7 8 9 10 11 12 13  \\documentclass{article} \\usepackage{tikz} %% 样式名称：dline；内容：颜色为蓝；线宽2pt \\tikzset{ dline/.style ={color = blue, line width =2pt} } \\begin{document} \\begin{tikzpicture} \\draw[dline] (0,0) --(9,0); \\end{tikzpicture} \\end{document}   局部样式 在\\begin{tikzpicture}后使用[]添加局部样式选项\n1 2 3 4 5 6 7 8  \\documentclass{article} \\usepackage{tikz} \\begin{document} %% 局部样式，名称：miao；内容：颜色为红色，线条宽度2pt \\begin{tikzpicture}[miao/.style ={color=red,line width =2pt}] \\draw[miao] (0,0) --(9,0); \\end{tikzpicture} \\end{document}   节点 (node) 确定节点的样式 1 2 3 4 5 6 7 8 9 10  \\tikzset{ box/.style ={ rectangle, %矩形节点  rounded corners =5pt, %圆角  minimum width =50pt, %最小宽度  minimum height =20pt, %最小高度  inner sep=5pt, %文字和边框的距离  draw=blue %边框颜色 } }      名称 选项 默认值 说明     内边距 inner sep .333em 文本与边框之间的距离   水平内边距 inner xsep .333em 文本与边框之间的距离（水平方向）   垂直内边距 inner ysep .333em 文本与边框之间的距离（垂直方向）   外边距 outer sep no 边框之外的无形边距   水平外边距 outer xsep .5pgflinewidth 边框之外的无形边距（水平方向）   垂直外边距 outer ysep .5pgflinewidth 边框之外的无形边距（垂直方向）   最小高度 minimum height 0pt 节点的最小高度，如果文本和内边距的高度小于最小高度，节点自动增大为最小高度   最小宽度 minimum width 0pt 节点的最小宽度，如果文本和内边距的高度小于最小宽度，节点自动增大为最小宽度   节点大小 minimum size no 节点最小大小，节点的高度和宽度自动大于该值   宽高比 shape aspect no 节点形状的宽高比   旋转节点 rotate no 旋转整个节点   旋转边框 shape border rotate no 旋转节点的边框          利用节点画流程图 1 2 3 4 5 6 7 8 9 10  \\begin{tikzpicture} \\filldraw[gray] (0,0) circle (.1); \\node[box] (b1) at(0,0) {A}; \\node[box] (b2) at(4,0) {B}; \\node[box] (b3) at(8,0) {C}; \\draw[-\u0026gt;] (b1)--(b2); \\draw[-\u0026gt;] (b2)--(b3); \\node at(2,1) {a}; \\node at(6,1) {b}; \\end{tikzpicture}    \\node[box] 指明了使用box风格的节点 (b1) 指明了节点的名字 at(0,0) 指明了节点的位置，位置为中心点 {A} 指明节点的文字  图中画了三个box节点，之后使用箭头连接起来。然后画了两个没有名字和边框的节点。\n子节点 使用 \\child 关键词来声明子节点。相邻节点之间的间距可以使用sibling distance控制。\n1 2 3 4 5 6 7 8 9  \\begin{tikzpicture}[sibling distance =80pt] \\node[box] {1} child { node[box] {2}} child { node[box] {3} child {node[box] {4}} child {node[box] {5}} child {node[box] {6}} }; \\end{tikzpicture}   插入外部图片 \\begin{tikzpicture} \\node at (0,0) {\\includegraphics[width=5cm,height=2cm]{pass.png}}; \\end{tikzpicture} 绘制函数 \\begin{tikzpicture} \\draw[-\u0026gt;] (-0.2,0) --(6,0) node[right] {$x$}; \\draw[-\u0026gt;] (0,-0.2) --(0,6) node[above] {$f(x)$}; \\draw[domain=0:4] plot (\\x,{0.1* exp(\\x)}) node[right] {$f(x)=\\frac{1}{10}e^x$}; \\end{tikzpicture}  domain = 0:4 设置了绘图的范围是0-4 plot 是绘制操作 node[right]{} 表示文字在箭头的右面，{}为文字内容  推荐pgfplots画函数：\nhttp://pgfplots.sourceforge.net/gallery.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \\documentclass{article} \\usepackage{pgfplots} \\begin{document} \\begin{tikzpicture} \\begin{axis}[ xlabel=Cost, ylabel=Error] \\addplot[color=red,mark=x] coordinates { (2,-2.8559703) (3,-3.5301677) (4,-4.3050655) (5,-5.1413136) (6,-6.0322865) (7,-6.9675052) (8,-7.9377747) }; \\end{axis} \\end{tikzpicture} \\end{document}   ","description":"","id":13,"section":"posts","tags":["作图","tikz","latex"],"title":"Tikz学习笔记","uri":"https://miaomiaoyoung.github.io/en/posts/tikz%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"content":"嘎  姑奶奶今天不把你打得桃花满面红，你就不知道姑奶奶心花为谁开。 ——郭芙蓉\n  数学是火，点亮计算机的灯；计算机是灯，照亮人工智能的路；人工智能是路，通向神经网络的坑；神经网络是坑，毁了我的前半生\n  如何证明哆啦A梦实现愿望的道具之间不存在等价性？\n  ┏┯┯┯┯┯┯┯┯┯┓\n┠車马象士将士象马車┨\n┠┼┼┼┼┼┼┼┼┼┨\n┠┼炮┼┼┼┼┼炮┼┨\n┠卒┼卒┼卒┼卒┼卒┨\n┠┴┴┴┴┴┴┴┴┴┨\n┠┬┬┬┬┬┬┬┬┬┨\n┠兵┼兵┼兵┼兵┼兵┨\n┠┼炮┼┼┼┼┼炮┼┨\n┠┼┼┼┼┼┼┼┼┼┨\n┠车马象士帅士象马车┨\n┗┷┷┷┷┷┷┷┷┷┛\n  王来搞笑，王来逗比，王来愉悦整个世界\n   ","description":"","id":14,"section":"posts","tags":["碎碎念"],"title":"细嗅蔷薇","uri":"https://miaomiaoyoung.github.io/en/posts/%E7%BB%86%E5%97%85%E8%94%B7%E8%96%87/"},{"content":" docker创建容器 中文编码 多显卡运行程序 ubuntu 镜像服务站 pip 镜像服务站 linux 文件数 linux 传输文件 ubuntu 版本 显示网速 tar  tar 仅打包 tar.gz 压缩打包    docker创建容器 1 2  nvidia-docker run -it --net=host --shm-size 6G -v /home/MiaoMiaoYang:/MiaoMiaoYang miaomiaoyang/pytorch:v12 ## --shm-size 指定容器内存的大小   中文编码 1 2  export LANG=C.UTF-8 source /etc/profile   多显卡运行程序 CUDA_VISIBLE_DEVICES=0,1 XXX\nubuntu 镜像服务站 1 2 3  cp /etc/apt/sources.list /etc/apt/sources.list.bak vim /etc/apt/sources.list apt-get update    https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n pip 镜像服务站  https://mirrors.tuna.tsinghua.edu.cn/help/pypi/\n pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\nlinux 文件数 1  ls -l | grep -c \u0026#39;^-\u0026#39;   linux 传输文件 1 2  scp local_file remote_username@remote_ip:remote_file scp -r local_folder remote_username@remote_ip:remote_folder   ubuntu 版本 1  cat /proc/version   显示网速 1 2  apt-get install nload nload -u H   tar tar 仅打包 1 2 3 4  ## 打包 tar -cvf code.tar.gz *.* m* r* ## 解压 tar -xvf code.tar.gz   tar.gz 压缩打包 1 2 3 4  ## 打包压缩 tar -zcvf code.tar.gz *.* m* r* ## 解压到当前目录下 tar -zxvf code.tar.gz   ","description":"","id":15,"section":"posts","tags":["linux","ubuntu","docker","综合"],"title":"linux","uri":"https://miaomiaoyoung.github.io/en/posts/linux/"},{"content":"参考：\n https://blog.csdn.net/hanchaobiao/article/details/84069299 https://blog.csdn.net/Thanours/article/details/109271836  平台：ubuntu 18.04\n启动docker容器，映射端口 1 2  nvidia-docker run -it --name nnunet --shm-size 6G -p 8000:80 -p 10022:22 -p 5000:5000 -v /home/MiaoMiaoYang:/MiaoMiaoYang miaomiaoyang/pytorch:nnunet ## --net=host存在的情况下，端口映射不管用，所以需要把网络模式更改一下   容器修改，建立ssh   修改root密码\n1  passwd:123456     安装ssh\n1 2  apt-get install openssh-server apt-get install openssh-client     检查是否启动\n1  ps -e |grep ssh   如果看到sshd说明已经启动了\n  允许root账户登录ssh\n1 2  vim /etc/ssh/sshd_config ## 注意，这里是sshd，还有一个类似的ssh是没有的   vim中使用/PermitRootLogin进行搜索，将下述字段替换\n#LoginGraceTime 2m #PermitRootLogin prohibit-password #StrictModes yes LoginGraceTime 2m PermitRootLogin yes StrictModes yes 然后，重启ssh使设置生效\n1  service ssh restart     测试容器是否能与外部通信\n1  ssh root@127.0.0.1 -p 10022     pycharm远程连接   设置pycharm\nPycharm $\\rightarrow$ Tools $\\rightarrow$ Deployment $\\rightarrow$ Configuration\n  配置SFTP远程服务器\n host: 服务器地址 Username: root (不是主机的user) Password: docker中root的密码 端口：10022 (docker中的端口)  点击Test Connection测试链接是否成功，测试成功如下图所示：\n  将本地代码与远程服务器代码连接\nMappings中，Local path为本地项目地址，Deployment path为远程服务器地址，点击OK\n  将代码上传到服务器进行调试测试\n  pycharm配置docker服务   配置Python编译器\nFile $\\rightarrow$ Settings，配置项目编译器（Project Interpreter），选择SSH Interpreter，选择现有服务器设定，Pycharm会自动选中之前STFP的设定。\n之后选择Move this server to IDE settings，之后Next。\n选择远程主机中Python的路径，然后Finish。\n（docker中可能没有sudo，就不需要选了）\n可以看到，已经加入了远程的编译器了\n配置Python运行设置\n  ","description":"","id":16,"section":"posts","tags":["pycharm","docker","ubuntu","linux"],"title":"Pycharm远程连接Docker容器","uri":"https://miaomiaoyoung.github.io/en/posts/pycharm%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5docker/"},{"content":"文件操作 1 2 3  # 递归删除一个目录以及目录内的所有内容（包括这个目录） shutil.rmtree( src ) # maybe_mkdir_p   pip cv2 1  pip install opencv-python   安装成功之后Docker ubuntu 中会报错：\n1 2  ImportError: libGL.so.1: cannot open shared object file: No such file or directory ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory   解决：\n1 2  apt-get install -y libgl1-mesa-dev apt-get install libglib2.0   ","description":"","id":17,"section":"posts","tags":["python"],"title":"Python","uri":"https://miaomiaoyoung.github.io/en/posts/python/"},{"content":"[TOC]\n 数据集：Decathlon Task07_Pancreas 平台： Ubuntu为主 (windows作测试用，其中很多不准) 参考：\n https://zhuanlan.zhihu.com/p/125730993 https://zhuanlan.zhihu.com/p/125630560 https://zhuanlan.zhihu.com/p/123901100 https://zhuanlan.zhihu.com/p/100014604  准备\u0026amp;Decathlon数据集使用\n 设定数据集路径 将Decathlon数据集进行转换 进行前处理 对数据进行整理（裁剪4D数据） 对数据集进行分析 对3D网络进行数据集前处理 planner_3d  进行训练\n 损失函数 生成训练数据集  更改（windows）\n  准备\u0026amp;Decathlon数据集使用 设定数据集路径 在环境路径中，设定网络的基本路径：\n [nnUNet_raw_data_base] - D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data [nnUNet_preprocessed] - D:\\Projects\\nnUNet\\data_base\\nnUNet_preprocessed [RESULTS_FOLDER] - D:\\Projects\\nnUNet\\data_base\\result_folder  1 2 3  export nnUNet_raw_data_base=\u0026#34;/MiaoMiaoYang/nnUNet/data/nnUNet_raw\u0026#34; export nnUNet_preprocessed=\u0026#34;/MiaoMiaoYang/nnUNet/data/nnUNet_preprocessed\u0026#34; export RESULTS_FOLDER=\u0026#34;/MiaoMiaoYang/nnUNet/data/result_folder\u0026#34;   将Decathlon数据集进行转换 因为有一些数据是4D时序的数据，比如心脏，所以在这里，他将decathlon数据集转换成统一格式。\n 如果是3D数据，则文件内容不变，更改文件名，Eg: pancreas_001.nii.gz $\\rightarrow$ pancreas_001_0000.nii.gz 如果是4D数据，则拆分成各个3D内容，在后面4位角标进行标注  1 2 3 4  nnUNet_convert_decathlon_task -i FOLDER_TO_TASK_AS_DOWNLOADED_FROM_MSD -p NUM_PROCESSES # Example:  nnUNet_convert_decathlon_task -i D:/Projects/nnUNet/data_base/nnUNet_raw_data/Task07_Pancreas -p 1   转换后，[nnUNet_raw_data_base]\\ nnUNet_raw_data\\ Task007_Pancreas 下的文件如下所示：\n|-- nnUNet_preprocessed |-- nnUNet_raw | |-- nnUNet_cropped_data | `-- nnUNet_raw_data | `-- Task007_Pancreas | |-- dataset.json | |-- imagesTr | | |-- pancreas_001_0000.nii.gz | | |-- pancreas_004_0000.nii.gz | | |-- ... | | `-- pancreas_421_0000.nii.gz | |-- imagesTs | | |-- pancreas_002_0000.nii.gz | | |-- pancreas_003_0000.nii.gz | | |-- ... | | `-- pancreas_420_0000.nii.gz | `-- labelsTr | |-- pancreas_001.nii.gz | |-- pancreas_004.nii.gz | |-- ... | `-- pancreas_421.nii.gz `-- result_folder `-- nnUNet 进行前处理 1 2 3 4  nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity ## Example nnUNet_plan_and_preprocess -t 007 --verify_dataset_integrity   对数据进行整理（裁剪4D数据） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  def crop(task_string, override=False, num_threads=default_num_threads): ## cropped_out_dir:D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data\\nnUNet_cropped_data\\Task007_Pancreas cropped_out_dir = join(nnUNet_cropped_data, task_string) os.makedirs(cropped_out_dir,exist_ok=True) if override and isdir(cropped_out_dir): shutil.rmtree(cropped_out_dir) maybe_mkdir_p(cropped_out_dir) ## splitted_4d_output_dir_task:D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data\\nnUNet_raw_data\\Task007_Pancreas splitted_4d_output_dir_task = join(nnUNet_raw_data, task_string) ## lists ## [[\u0026#39;D:\\\\Projects\\\\nnUNet\\\\data_base\\\\nnUNet_raw_data\\\\nnUNet_raw_data\\\\Task007_Pancreas\\\\imagesTr\\\\pancreas_290_0000.nii.gz\u0026#39;, ## \u0026#39;D:\\\\Projects\\\\nnUNet\\\\data_base\\\\nnUNet_raw_data\\\\nnUNet_raw_data\\\\Task007_Pancreas\\\\labelsTr\\\\pancreas_290.nii.gz\u0026#39;], ## ...] lists, _ = create_lists_from_splitted_dataset(splitted_4d_output_dir_task) ## nnunet \u0026gt; preprocessing \u0026gt; cropping.py \u0026gt; [class ImageCropper] imgcrop = ImageCropper(num_threads, cropped_out_dir) imgcrop.run_cropping(lists, overwrite_existing=override) shutil.copy(join(nnUNet_raw_data, task_string, \u0026#34;dataset.json\u0026#34;), cropped_out_dir)   在 [nnUNet_raw_data_base] \\nnUNet_cropped_data \\Task007_Pancreas 下生成了图像(.npz，包含图像和标注)和他的属性信息(.pkl)，并且生成gt_segmentation保存分割的金标准\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  ## Example: pancreas_290.npz data = np.load(r\u0026#34;D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data\\nnUNet_raw_data\\Task007_Pancreas\\imagesTr\\pancreas_290.npz\u0026#34;) print(data[\u0026#39;data\u0026#39;].shape) ## Output: (2,97,512,512) ----------------------------------------------- import pickle from pprint import pprint with open(r\u0026#34;D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data\\nnUNet_raw_data\\Task007_Pancreas\\imagesTr\\pancreas_290.pkl\u0026#34;,\u0026#39;rb\u0026#39;) as f: data = pickle.load(f) pprint(data) ## Output: # OrderedDict([(\u0026#39;original_size_of_raw_data\u0026#39;, array([ 97, 512, 512])), # (\u0026#39;original_spacing\u0026#39;, array([2.5 , 0.91601598, 0.91601598])), # (\u0026#39;list_of_data_files\u0026#39;, # [\u0026#39;D:\\\\Projects\\\\nnUNet\\\\data_base\\\\nnUNet_raw_data\\\\nnUNet_raw_data\\\\Task007_Pancreas\\\\imagesTr\\\\pancreas_290_0000.nii.gz\u0026#39;]), # (\u0026#39;seg_file\u0026#39;, # \u0026#39;D:\\\\Projects\\\\nnUNet\\\\data_base\\\\nnUNet_raw_data\\\\nnUNet_raw_data\\\\Task007_Pancreas\\\\labelsTr\\\\pancreas_290.nii.gz\u0026#39;), # (\u0026#39;itk_origin\u0026#39;, (468.08416748046875, 468.08416748046875, 0.0)), # (\u0026#39;itk_spacing\u0026#39;, (0.9160159826278687, 0.9160159826278687, 2.5)), # (\u0026#39;itk_direction\u0026#39;, (-1.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 1.0)), # (\u0026#39;crop_bbox\u0026#39;, [[0, 97], [0, 512], [0, 512]]), # (\u0026#39;classes\u0026#39;, array([-1., 0., 1., 2.], dtype=float32)), # (\u0026#39;size_after_cropping\u0026#39;, (97, 512, 512))])   结束后的文件加是\n|-- nnUNet_preprocessed | |-- nnUNet_raw | | | |-- nnUNet_cropped_data | | | | | `-- Task007_Pancreas | | | | | |-- dataset.json | | | | | |-- gt_segmentations | | | |-- pancreas_001.nii.gz | | | |-- pancreas_004.nii.gz | | | |-- ... | | | `-- pancreas_421.nii.gz | | | | | |-- pancreas_001.npz | | |-- pancreas_001.pkl | | |-- pancreas_004.npz | | |-- pancreas_004.pkl | | |-- ... .npz | | |-- ... .pkl | | |-- pancreas_421.npz | | `-- pancreas_421.pkl | | | `-- nnUNet_raw_data | | | `-- Task007_Pancreas | | | |-- dataset.json | | | |-- imagesTr | | |-- pancreas_001_0000.nii.gz | | |-- pancreas_004_0000.nii.gz | | |-- ... | | `-- pancreas_421_0000.nii.gz | | | |-- imagesTs | | |-- pancreas_002_0000.nii.gz | | |-- pancreas_003_0000.nii.gz | | |-- ... | | `-- pancreas_420_0000.nii.gz | | | `-- labelsTr | |-- pancreas_001.nii.gz | |-- pancreas_004.nii.gz | |-- ... | `-- pancreas_421.nii.gz | `-- result_folder | `-- nnUNet  在这个过程中可能会出现报错“ multiprocessing.pool.MaybeEncoding Error: Error sending result: ''. Reason: 'PicklingError(\u0026quot;Can't pickle : it's not the same object as builtins.MemoryError\u0026quot;)'” 原因是在这个过程中RAM爆了，所以多跑几次把这个跑完就好了\n 对数据集进行分析 1 2  dataset_analyzer = DatasetAnalyzer(cropped_out_dir, overwrite=False, num_processes=tf) # this class creates the fingerprint _ = dataset_analyzer.analyze_dataset(collect_intensityproperties) # this will write output files that will be used by the ExperimentPlanner   进行数据集分析，储存在 [nnUNet_raw_data_base]\\ nnUNet_cropped_data\\ Task007_Pancreas\\ dataset_properties.pkl中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  import pickle from pprint import pprint with open(r\u0026#34;D:\\Projects\\nnUNet\\data_base\\nnUNet_raw_data\\nnUNet_cropped_data\\Task007_Pancreas\\dataset_properties.pkl\u0026#34;,\u0026#39;rb\u0026#39;) as f: data = pickle.load(f) pprint(data) # {\u0026#39;all_classes\u0026#39;: [1, 2], # \u0026#39;all_sizes\u0026#39;: [(110, 512, 512), # (107, 512, 512), # ... # (131, 512, 512)], # \u0026#39;all_spacings\u0026#39;: [array([2.5 , 0.64453101, 0.64453101]), # array([1.5 , 0.97656202, 0.97656202]), # ... # array([2. , 0.97265601, 0.97265601])], # \u0026#39;intensityproperties\u0026#39;: OrderedDict([(0, # OrderedDict([(\u0026#39;local_props\u0026#39;, # OrderedDict([(\u0026#39;pancreas_001\u0026#39;, # OrderedDict([(\u0026#39;median\u0026#39;,84.0), # (\u0026#39;mean\u0026#39;,95.87825), # (\u0026#39;sd\u0026#39;,179.51465), # (\u0026#39;mn\u0026#39;,-71.0), # (\u0026#39;mx\u0026#39;,3034.0), # (\u0026#39;percentile_99_5\u0026#39;,1569.2499999999654), # (\u0026#39;percentile_00_5\u0026#39;,-41.15)])), # (\u0026#39;pancreas_004\u0026#39;, # OrderedDict([(\u0026#39;median\u0026#39;,78.0), # (\u0026#39;mean\u0026#39;,72.81623), # (\u0026#39;sd\u0026#39;,124.151405), # (\u0026#39;mn\u0026#39;,-996.0), # (\u0026#39;mx\u0026#39;,2226.0), # (\u0026#39;percentile_99_5\u0026#39;,729.9399999999987), # (\u0026#39;percentile_00_5\u0026#39;,-114.0)])), # (\u0026#39;pancreas_421\u0026#39;, # OrderedDict([(\u0026#39;median\u0026#39;,62.0), # (\u0026#39;mean\u0026#39;,52.686794), # (\u0026#39;sd\u0026#39;,45.905533), # (\u0026#39;mn\u0026#39;,-126.0), # (\u0026#39;mx\u0026#39;,186.0), # (\u0026#39;percentile_99_5\u0026#39;,140.0), # (\u0026#39;percentile_00_5\u0026#39;,-103.515)]))])), # (\u0026#39;median\u0026#39;, 84.0), # (\u0026#39;mean\u0026#39;, 77.98502), # (\u0026#39;sd\u0026#39;, 75.32122), # (\u0026#39;mn\u0026#39;, -996.0), # (\u0026#39;mx\u0026#39;, 3071.0), # (\u0026#39;percentile_99_5\u0026#39;, 215.0), # (\u0026#39;percentile_00_5\u0026#39;,-96.0)]))]), # \u0026#39;modalities\u0026#39;: {0: \u0026#39;CT\u0026#39;}, # \u0026#39;size_reductions\u0026#39;: OrderedDict([(\u0026#39;pancreas_001\u0026#39;, 1.0), # (\u0026#39;pancreas_004\u0026#39;, 1.0), # ..., # (\u0026#39;pancreas_421\u0026#39;, 1.0)])}   之后将dataset_properties.pkl，还有dataset.json拷贝到了 [nnUNet_preprocessed]/Task007_Pancreas 中\n对3D网络进行数据集前处理 planner_3d nnunet $\\rightarrow$ experiment_planning $\\rightarrow$ experiment_planner_baseline_3DUNet.py\n 归一化 除CT外，其他模态使用的归一化都是z-scoring\nCT使用的是全局归一化. To this end, nnU-Net uses the 0.5 and 99.5 percentiles of the foreground voxels for clipping as well as the global foreground mean a standard deviation for normalization on all images\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  def determine_whether_to_use_mask_for_norm(self): # only use the nonzero mask for normalization of the cropping based on it resulted in a decrease in # image size (this is an indication that the data is something like brats/isles and then we want to # normalize in the brain region only) modalities = self.dataset_properties[\u0026#39;modalities\u0026#39;] num_modalities = len(list(modalities.keys())) use_nonzero_mask_for_norm = OrderedDict() for i in range(num_modalities): if \u0026#34;CT\u0026#34; in modalities[i]: use_nonzero_mask_for_norm[i] = False else: all_size_reductions = [] for k in self.dataset_properties[\u0026#39;size_reductions\u0026#39;].keys(): all_size_reductions.append(self.dataset_properties[\u0026#39;size_reductions\u0026#39;][k]) if np.median(all_size_reductions) \u0026lt; 3 / 4.: print(\u0026#34;using nonzero mask for normalization\u0026#34;) use_nonzero_mask_for_norm[i] = True else: print(\u0026#34;not using nonzero mask for normalization\u0026#34;) use_nonzero_mask_for_norm[i] = False for c in self.list_of_cropped_npz_files: case_identifier = get_case_identifier_from_npz(c) properties = self.load_properties_of_cropped(case_identifier) properties[\u0026#39;use_nonzero_mask_for_norm\u0026#39;] = use_nonzero_mask_for_norm self.save_properties_of_cropped(case_identifier, properties) use_nonzero_mask_for_normalization = use_nonzero_mask_for_norm return use_nonzero_mask_for_normalization   寻找目标的CT间隔(spacings)\n1 2 3 4 5 6 7 8  def get_target_spacing(self): spacings = self.dataset_properties[\u0026#39;all_spacings\u0026#39;] # target = np.median(np.vstack(spacings), 0) # if target spacing is very anisotropic we may want to not downsample the axis with the worst spacing # uncomment after mystery task submission # 在初始化中设定：self.target_spacing_percentile = 50 target = np.percentile(np.vstack(spacings), self.target_spacing_percentile, 0) return target   将所有CT的中位数作为所有CT的目标spacing，之后利用新的spacing得到进行归一化的图像shape:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  target_spacing = self.get_target_spacing() new_shapes = [np.array(i) / target_spacing * np.array(j) for i, j in zip(spacings, sizes)] print(\u0026#34;target_spacing:\u0026#34;) print(target_spacing) print(\u0026#34;sizes:\u0026#34;) print(sizes) print(\u0026#34;new_shapes:\u0026#34;) print(new_shapes) # target_spacing: # [2.5, 0.80273402, 0.80273402] # sizes: # [(110, 512, 512), (107, 512, 512), ..., (131, 512, 512)] # new_shapes: # [([110., 411.09492159, 411.09492159]), ([107., 473.38251017, 473.38251017]),...,([104.8, 620.37968636, 620.37968636])]   可以看到，这里图像大小归一化的策略是根据spacing对图像的所有维度进行归一化，而不是只针对axial轴进行归一化。之后他进行了spacing的大小分析：\n1 2 3 4 5 6 7 8 9 10 11 12  # we base our calculations on the median shape of the datasets median_shape = np.median(np.vstack(new_shapes), 0) print(\u0026#34;the median shape of the dataset is \u0026#34;, median_shape) max_shape = np.max(np.vstack(new_shapes), 0) print(\u0026#34;the max shape in the dataset is \u0026#34;, max_shape) min_shape = np.min(np.vstack(new_shapes), 0) print(\u0026#34;the min shape in the dataset is \u0026#34;, min_shape) print(\u0026#34;we don\u0026#39;t want feature maps smaller than \u0026#34;, selfunet_featuremap_min_edge_length, \u0026#34; in the bottleneck\u0026#34;) # the median shape of the dataset is [96. 512. 512.]  # the max shape in the dataset is [210.28361328 622.87101959 622.87101959] # the min shape in the dataset is [51. 386.18037278 386.18037278]  # we don\u0026#39;t want feature maps smaller than 4 in the bottleneck   自动计算训练策略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  print(\u0026#34;generating configuration for 3d_fullres\u0026#34;) self.plans_per_stage.append(self.get_properties_for_stage( target_spacing_transposed, target_spacing_transposed, median_shape_transposed, len(self.list_of_cropped_npz_files), num_modalities, len(all_classes) + 1 ) print(self.plans_per_stage) # [{\u0026#39;batch_size\u0026#39;: 2, # \u0026#39;num_pool_per_axis\u0026#39;: [3, 5, 5],  # \u0026#39;patch_size\u0026#39;: array([40, 224, 224]), # \u0026#39;median_patient_size_in_voxels\u0026#39;: array([96, 512, 512]), # \u0026#39;current_spacing\u0026#39;: array([2.5, 0.80273402, 0.80273402]), # \u0026#39;original_spacing\u0026#39;: array([2.5, 0.80273402, 0.80273402]),  # \u0026#39;do_dummy_2D_data_aug\u0026#39;: True, # \u0026#39;pool_op_kernel_sizes\u0026#39;: [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], # \u0026#39;conv_kernel_sizes\u0026#39;: [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]}]   注意，这里的def get_properties_for_stage跳转到了experiments_planner_baseline_3DUNet.py中了\n根据spacing计算，如果spacing最大的方向的体素是512，那么这个feature map的shape是多少，然后和之前选中的那个shape进行比较，选小的一边\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  \u0026#34;\u0026#34;\u0026#34; current_spacing: [2.5 0.80273402 0.80273402] new_median_shape: [96 512 512] \u0026#34;\u0026#34;\u0026#34; # compute how many voxels are one mm # input_path_size [0.4 1.24574265 1.24574265] input_patch_size = 1 / np.array(current_spacing) # normalize voxels per mm # 这个归一化我其实不是很懂，是不是将path等比例缩放，因为后面是1/min() # 或者我觉得更有道理的normalize应该是对mm^3进行归一化 # input_path_size [0.41501162 1.29249419 1.29249419] input_patch_size /= input_patch_size.mean() # create an isotropic patch of size 512x512x512mm # 这里的意思其实是，如果建立一个各个方向spacing都是一样的，需要的voxels，也就是map的大小， # 这里设定了最小的一个方向的大小是 512 voxel # input_path_size [ 512. 1594.55058874 1594.55058874] input_patch_size *= 1 / min(input_patch_size) * 512 # to get a starting value # input_path_size [ 512 1595 1595] input_patch_size = np.round(input_patch_size).astype(int) # clip it to the median shape of the dataset because patches larger then that make not much sense # 各个方向上取一个最小的结果 # input_path_size [ 96 512 512] input_patch_size = [min(i, j) for i, j in zip(input_patch_size, new_median_shape)]   根据输入的大小，计算网络结构以及可除尽的大小\n1 2 3 4 5 6 7 8 9 10 11  network_num_pool_per_axis, pool_op_kernel_sizes, conv_kernel_sizes, new_shp, \\ shape_must_be_divisible_by = get_pool_and_conv_props( current_spacing, input_patch_size, self.unet_featuremap_min_edge_length, self.unet_max_numpool) ## network_num_pool_per_axis: [4, 7, 7] ## pool_op_kernel_sizes: [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]] ## conv_kernel_sizes: [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] ##new_shape: [ 96 512 512] ##shape_must_be_divisible_by: [ 16 128 128]   自动计算网络架构：\n网络中仅使用conv - \u0026gt;instance normalization -\u0026gt; Leaky relu。上采样使用的是转置卷积。确定好input_path_size后，对每个方向进行下采样，直到最终的feature map size小于 $4\\times 4\\times 4$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  def get_pool_and_conv_props(spacing, patch_size, min_feature_map_size, max_numpool): \u0026#34;\u0026#34;\u0026#34; :param spacing: :param patch_size: :param min_feature_map_size: min edge length of feature maps in bottleneck \u0026#34;\u0026#34;\u0026#34; dim = len(spacing) current_spacing = deepcopy(list(spacing)) current_size = deepcopy(list(patch_size)) pool_op_kernel_sizes = [] conv_kernel_sizes = [] num_pool_per_axis = [0] * dim while True: # This is a problem because sometimes we have spacing 20, 50, 50 and we want to still keep pooling. # Here we would stop however. This is not what we want! Fixed in get_pool_and_conv_propsv2 min_spacing = min(current_spacing) valid_axes_for_pool = [i for i in range(dim) if current_spacing[i] / min_spacing \u0026lt; 2] axes = [] for a in range(dim): my_spacing = current_spacing[a] partners = [i for i in range(dim) if current_spacing[i] / my_spacing \u0026lt; 2 and my_spacing / current_spacing[i] \u0026lt; 2] if len(partners) \u0026gt; len(axes): axes = partners conv_kernel_size = [3 if i in axes else 1 for i in range(dim)] # exclude axes that we cannot pool further because of min_feature_map_size constraint #before = len(valid_axes_for_pool) valid_axes_for_pool = [i for i in valid_axes_for_pool if current_size[i] \u0026gt;= 2*min_feature_map_size] #after = len(valid_axes_for_pool) #if after == 1 and before \u0026gt; 1: # break  valid_axes_for_pool = [i for i in valid_axes_for_pool if num_pool_per_axis[i] \u0026lt; max_numpool] if len(valid_axes_for_pool) == 0: break #print(current_spacing, current_size)  other_axes = [i for i in range(dim) if i not in valid_axes_for_pool] pool_kernel_sizes = [0] * dim for v in valid_axes_for_pool: pool_kernel_sizes[v] = 2 num_pool_per_axis[v] += 1 current_spacing[v] *= 2 current_size[v] = np.ceil(current_size[v] / 2) for nv in other_axes: pool_kernel_sizes[nv] = 1 pool_op_kernel_sizes.append(pool_kernel_sizes) conv_kernel_sizes.append(conv_kernel_size) #print(conv_kernel_sizes)  must_be_divisible_by = get_shape_must_be_divisible_by(num_pool_per_axis) patch_size = pad_shape(patch_size, must_be_divisible_by) # we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here conv_kernel_sizes.append([3]*dim) return num_pool_per_axis, pool_op_kernel_sizes, conv_kernel_sizes, patch_size, must_be_divisible_by   实施预处理\nnnunet $\\rightarrow$ preprocessing $\\rightarrow$ preprocessing.py $\\rightarrow$ class GenericPreprocessor\n主要运行函数是def _run_internal\n 加载cropped数据，之后进行resample\n这里的resample是根据target spacing对数据和标签进行重采样到指定的spacing\n然后进行数值的调整，clip掉$HU \\in [... ,0.5\\%] \u0026amp; [99.5\\%, ...]$ 然后对HU进行z-score归一化\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None): \u0026#34;\u0026#34;\u0026#34; data and seg must already have been transposed by transpose_forward. properties are the un-transposed values (spacing etc) :param data: :param target_spacing: :param properties: :param seg: :param force_separate_z: :return: \u0026#34;\u0026#34;\u0026#34; # target_spacing is already transposed, properties[\u0026#34;original_spacing\u0026#34;] is not so we need to transpose it! # data, seg are already transposed. Double check this using the properties original_spacing_transposed = np.array(properties[\u0026#34;original_spacing\u0026#34;])[self.transpose_forward] before = { \u0026#39;spacing\u0026#39;: properties[\u0026#34;original_spacing\u0026#34;], \u0026#39;spacing_transposed\u0026#39;: original_spacing_transposed, \u0026#39;data.shape (data is transposed)\u0026#39;: data.shape } # remove nans data[np.isnan(data)] = 0 print(\u0026#39;data shape: {}\u0026#39;.format(data.shape)) data, seg = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold) after = { \u0026#39;spacing\u0026#39;: target_spacing, \u0026#39;data.shape (data is resampled)\u0026#39;: data.shape } print(\u0026#34;before:\u0026#34;, before, \u0026#34;\\nafter: \u0026#34;, after, \u0026#34;\\n\u0026#34;) if seg is not None: # hippocampus 243 has one voxel with -2 as label. wtf? seg[seg \u0026lt; -1] = 0 properties[\u0026#34;size_after_resampling\u0026#34;] = data[0].shape properties[\u0026#34;spacing_after_resampling\u0026#34;] = target_spacing use_nonzero_mask = self.use_nonzero_mask assert len(self.normalization_scheme_per_modality) == len(data), \u0026#34;self.normalization_scheme_per_modality \u0026#34; \\ \u0026#34;must have as many entries as data has \u0026#34; \\ \u0026#34;modalities\u0026#34; assert len(self.use_nonzero_mask) == len(data), \u0026#34;self.use_nonzero_mask must have as many entries as data\u0026#34; \\ \u0026#34; has modalities\u0026#34; for c in range(len(data)): scheme = self.normalization_scheme_per_modality[c] if scheme == \u0026#34;CT\u0026#34;: # clip to lb and ub from train data foreground and use foreground mn and sd from training data assert self.intensityproperties is not None, \u0026#34;ERROR: if there is a CT then we need intensity properties\u0026#34; mean_intensity = self.intensityproperties[c][\u0026#39;mean\u0026#39;] std_intensity = self.intensityproperties[c][\u0026#39;sd\u0026#39;] lower_bound = self.intensityproperties[c][\u0026#39;percentile_00_5\u0026#39;] upper_bound = self.intensityproperties[c][\u0026#39;percentile_99_5\u0026#39;] data[c] = np.clip(data[c], lower_bound, upper_bound) data[c] = (data[c] - mean_intensity) / std_intensity if use_nonzero_mask[c]: data[c][seg[-1] \u0026lt; 0] = 0 elif scheme == \u0026#34;CT2\u0026#34;: # clip to lb and ub from train data foreground, use mn and sd form each case for normalization assert self.intensityproperties is not None, \u0026#34;ERROR: if there is a CT then we need intensity properties\u0026#34; lower_bound = self.intensityproperties[c][\u0026#39;percentile_00_5\u0026#39;] upper_bound = self.intensityproperties[c][\u0026#39;percentile_99_5\u0026#39;] mask = (data[c] \u0026gt; lower_bound) \u0026amp; (data[c] \u0026lt; upper_bound) data[c] = np.clip(data[c], lower_bound, upper_bound) mn = data[c][mask].mean() sd = data[c][mask].std() data[c] = (data[c] - mn) / sd if use_nonzero_mask[c]: data[c][seg[-1] \u0026lt; 0] = 0 else: if use_nonzero_mask[c]: mask = seg[-1] \u0026gt;= 0 else: mask = np.ones(seg.shape[1:], dtype=bool) data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-8) data[c][mask == 0] = 0 return data, seg, properties    标签随机选点，平衡标签\n这种平衡标签的方式真的是特别amazing，之前从来没想过可以这么平衡标签\n他把label中的所有坐标点提取出来，然后进行随机的选点，设定随机的点最多10000个，最小不能少于标签坐标的1%。 这种的话就不需要对图像进行裁切来进行平衡，如果有斑驳的话也可以使用后期的出来来进行弥补\n最后data['class_locations']就是[num,3]的数组，保存了选择的label的点\n 但是这个10000的选择不知道适不适用于其他的任何数据集？？？\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # we need to find out where the classes are and sample some random locations # let\u0026#39;s do 10.000 samples per class # seed this for reproducibility! num_samples = 10000 min_percent_coverage = 0.01 # at least 1% of the class voxels need to be selected, otherwise it may be too sparse rndst = np.random.RandomState(1234) class_locs = {} for c in all_classes: all_locs = np.argwhere(all_data[-1] == c) if len(all_locs) == 0: class_locs[c] = [] continue target_num_samples = min(num_samples, len(all_locs)) target_num_samples = max(target_num_samples, int(np.ceil(len(all_locs) * min_percent_coverage))) selected = all_locs[rndst.choice(len(all_locs), target_num_samples, replace=False)] class_locs[c] = selected print(c, target_num_samples) properties[\u0026#39;class_locations\u0026#39;] = class_locs   两阶段Cascade Unet\n第一阶段unet是小图像 [107, 225, 225]，第二阶段是全图像 [110, 411, 411]。 但是每个阶段的图像都是全景图像。\n\n   进行训练  使用 3D U-Net Cascade 进行训练\n这一部分的代码在 nnunet -\u0026gt; run -\u0026gt; run_training.py 中\n  3D low resolution U-Net For FOLD in [0, 1, 2, 3, 4], run:\n1 2 3  nnUNet_train 3d_lowres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz ## Example nnUNet_train 3d_lowres nnUNetTrainerV2 Task007_Pancreas 0 --npz   3D full resolution U-Net For FOLD in [0, 1, 2, 3, 4], run:\n1 2 3  nnUNet_train 3d_cascade_fullres nnUNetTrainerV2CascadeFullRes TaskXXX_MYTASK FOLD --npz ## Example nnUNet_train 3d_cascade_fullres nnUNetTrainerV2CascadeFullRes Task007_Pancreas 0 --npz    损失函数  nnunet -\u0026gt; training -\u0026gt; loss_function -\u0026gt; dice_loss.py\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  class DC_and_CE_loss(nn.Module): def __init__(self, soft_dice_kwargs, ce_kwargs, aggregate=\u0026#34;sum\u0026#34;, square_dice=False, weight_ce=1, weight_dice=1, log_dice=False, ignore_label=None): \u0026#34;\u0026#34;\u0026#34; CAREFUL. Weights for CE and Dice do not need to sum to one. You can set whatever you want. :param soft_dice_kwargs: :param ce_kwargs: :param aggregate: :param square_dice: :param weight_ce: :param weight_dice: \u0026#34;\u0026#34;\u0026#34; super(DC_and_CE_loss, self).__init__() if ignore_label is not None: assert not square_dice, \u0026#39;not implemented\u0026#39; ce_kwargs[\u0026#39;reduction\u0026#39;] = \u0026#39;none\u0026#39; self.log_dice = log_dice self.weight_dice = weight_dice self.weight_ce = weight_ce self.aggregate = aggregate self.ce = RobustCrossEntropyLoss(**ce_kwargs) self.ignore_label = ignore_label if not square_dice: self.dc = SoftDiceLoss(apply_nonlin=softmax_helper, **soft_dice_kwargs) else: self.dc = SoftDiceLossSquared(apply_nonlin=softmax_helper, **soft_dice_kwargs) def forward(self, net_output, target): \u0026#34;\u0026#34;\u0026#34; target must be b, c, x, y(, z) with c=1 :param net_output: :param target: :return: \u0026#34;\u0026#34;\u0026#34; if self.ignore_label is not None: assert target.shape[1] == 1, \u0026#39;not implemented for one hot encoding\u0026#39; mask = target != self.ignore_label target[~mask] = 0 mask = mask.float() else: mask = None dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0 if self.log_dice: dc_loss = -torch.log(-dc_loss) ce_loss = self.ce(net_output, target[:, 0].long()) if self.weight_ce != 0 else 0 if self.ignore_label is not None: ce_loss *= mask[:, 0] ce_loss = ce_loss.sum() / mask.sum() if self.aggregate == \u0026#34;sum\u0026#34;: result = self.weight_ce * ce_loss + self.weight_dice * dc_loss else: raise NotImplementedError(\u0026#34;nah son\u0026#34;) # reserved for other stuff (later) return result   使用方法：\n1 2 3 4  \u0026#34;\u0026#34;\u0026#34; self.batch_size: False \u0026#34;\u0026#34;\u0026#34; loss = DC_and_CE_loss({\u0026#39;batch_dice\u0026#39;: self.batch_dice, \u0026#39;smooth\u0026#39;: 1e-5, \u0026#39;do_bg\u0026#39;: False}, {})   如果是多个标签，会使用权重进行分配，在MultiOutputLoss2中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  class MultipleOutputLoss2(nn.Module): def __init__(self, loss, weight_factors=None): \u0026#34;\u0026#34;\u0026#34; use this if you have several outputs and ground truth (both list of same len) and the loss should be computed between them (x[0] and y[0], x[1] and y[1] etc) :param loss: :param weight_factors: \u0026#34;\u0026#34;\u0026#34; super(MultipleOutputLoss2, self).__init__() self.weight_factors = weight_factors self.loss = loss def forward(self, x, y): assert isinstance(x, (tuple, list)), \u0026#34;x must be either tuple or list\u0026#34; assert isinstance(y, (tuple, list)), \u0026#34;y must be either tuple or list\u0026#34; if self.weight_factors is None: weights = [1] * len(x) else: weights = self.weight_factors l = weights[0] * self.loss(x[0], y[0]) for i in range(1, len(x)): if weights[i] != 0: l += weights[i] * self.loss(x[i], y[i]) return l   生成训练数据集  nnunet -\u0026gt; training -\u0026gt; network_training -\u0026gt; nnUNetTrainerV2.py\n 1 2  if training: self.dl_tr, self.dl_val = self.get_basic_generators()    nnunet -\u0026gt; training -\u0026gt; network_training -\u0026gt; nnUNetTrainer.py\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  def load_dataset(self): self.dataset = load_dataset(self.folder_with_preprocessed_data) def get_basic_generators(self): ## 加载数据集的基本情况 self.load_dataset() ## 训练集和测试集的分割 self.do_split() if self.threeD: dl_tr = DataLoader3D(self.dataset_tr, self.basic_generator_patch_size, self.patch_size, self.batch_size, False, oversample_foreground_percent=self.oversample_foreground_percent, pad_mode=\u0026#34;constant\u0026#34;, pad_sides=self.pad_all_sides, memmap_mode=\u0026#39;r\u0026#39;) dl_val = DataLoader3D(self.dataset_val, self.patch_size, self.patch_size, self.batch_size, False, oversample_foreground_percent=self.oversample_foreground_percent, pad_mode=\u0026#34;constant\u0026#34;, pad_sides=self.pad_all_sides, memmap_mode=\u0026#39;r\u0026#39;) else: dl_tr = DataLoader2D(self.dataset_tr, self.basic_generator_patch_size, self.patch_size, self.batch_size, oversample_foreground_percent=self.oversample_foreground_percent, pad_mode=\u0026#34;constant\u0026#34;, pad_sides=self.pad_all_sides, memmap_mode=\u0026#39;r\u0026#39;) dl_val = DataLoader2D(self.dataset_val, self.patch_size, self.patch_size, self.batch_size, oversample_foreground_percent=self.oversample_foreground_percent, pad_mode=\u0026#34;constant\u0026#34;, pad_sides=self.pad_all_sides, memmap_mode=\u0026#39;r\u0026#39;) return dl_tr, dl_val    nnunet -\u0026gt; training -\u0026gt; dataloading -\u0026gt; dataloading.py -\u0026gt; def load_dataset()\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def load_dataset(folder, num_cases_properties_loading_threshold=1000): # we don\u0026#39;t load the actual data but instead return the filename to the np file. print(\u0026#39;loading dataset\u0026#39;) case_identifiers = get_case_identifiers(folder) case_identifiers.sort() dataset = OrderedDict() for c in case_identifiers: dataset[c] = OrderedDict() dataset[c][\u0026#39;data_file\u0026#39;] = join(folder, \u0026#34;%s.npz\u0026#34; % c) # dataset[c][\u0026#39;properties\u0026#39;] = load_pickle(join(folder, \u0026#34;%s.pkl\u0026#34; % c)) dataset[c][\u0026#39;properties_file\u0026#39;] = join(folder, \u0026#34;%s.pkl\u0026#34; % c) if dataset[c].get(\u0026#39;seg_from_prev_stage_file\u0026#39;) is not None: dataset[c][\u0026#39;seg_from_prev_stage_file\u0026#39;] = join(folder, \u0026#34;%s_segs.npz\u0026#34; % c) if len(case_identifiers) \u0026lt;= num_cases_properties_loading_threshold: print(\u0026#39;loading all case properties\u0026#39;) for i in dataset.keys(): dataset[i][\u0026#39;properties\u0026#39;] = load_pickle(dataset[i][\u0026#39;properties_file\u0026#39;]) return dataset    nnunet -\u0026gt; training -\u0026gt; dataloading -\u0026gt; dataloading.py -\u0026gt; class DataLoader3D()\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225  class DataLoader3D(SlimDataLoaderBase): def __init__(self, data, patch_size, final_patch_size, batch_size, has_prev_stage=False, oversample_foreground_percent=0.0, memmap_mode=\u0026#34;r\u0026#34;, pad_mode=\u0026#34;edge\u0026#34;, pad_kwargs_data=None, pad_sides=None): \u0026#34;\u0026#34;\u0026#34; This is the basic data loader for 3D networks. It uses preprocessed data as produced by my (Fabian) preprocessing. You can load the data with load_dataset(folder) where folder is the folder where the npz files are located. If there are only npz files present in that folder, the data loader will unpack them on the fly. This may take a while and increase CPU usage. Therefore, I advise you to call unpack_dataset(folder) first, which will unpack all npz to npy. Don\u0026#39;t forget to call delete_npy(folder) after you are done with training? Why all the hassle? Well the decathlon dataset is huge. Using npy for everything will consume \u0026gt;1 TB and that is uncool given that I (Fabian) will have to store that permanently on /datasets and my local computer. With this strategy all data is stored in a compressed format (factor 10 smaller) and only unpacked when needed. :param data: get this with load_dataset(folder, stage=0). Plug the return value in here and you are g2g (good to go) :param patch_size: what patch size will this data loader return? it is common practice to first load larger patches so that a central crop after data augmentation can be done to reduce border artifacts. If unsure, use get_patch_size() from data_augmentation.default_data_augmentation :param final_patch_size: what will the patch finally be cropped to (after data augmentation)? this is the patch size that goes into your network. We need this here because we will pad patients in here so that patches at the border of patients are sampled properly :param batch_size: :param num_batches: how many batches will the data loader produce before stopping? None=endless :param seed: :param stage: ignore this (Fabian only) :param random: Sample keys randomly; CAREFUL! non-random sampling requires batch_size=1, otherwise you will iterate batch_size times over the dataset :param oversample_foreground: half the batch will be forced to contain at least some foreground (equal prob for each of the foreground classes) \u0026#34;\u0026#34;\u0026#34; super(DataLoader3D, self).__init__(data, batch_size, None) if pad_kwargs_data is None: pad_kwargs_data = OrderedDict() self.pad_kwargs_data = pad_kwargs_data self.pad_mode = pad_mode self.oversample_foreground_percent = oversample_foreground_percent self.final_patch_size = final_patch_size self.has_prev_stage = has_prev_stage self.patch_size = patch_size self.list_of_keys = list(self._data.keys()) # need_to_pad denotes by how much we need to pad the data so that if we sample a patch of size final_patch_size # (which is what the network will get) these patches will also cover the border of the patients self.need_to_pad = (np.array(patch_size) - np.array(final_patch_size)).astype(int) if pad_sides is not None: if not isinstance(pad_sides, np.ndarray): pad_sides = np.array(pad_sides) self.need_to_pad += pad_sides self.memmap_mode = memmap_mode self.num_channels = None self.pad_sides = pad_sides self.data_shape, self.seg_shape = self.determine_shapes() def get_do_oversample(self, batch_idx): return not batch_idx \u0026lt; round(self.batch_size * (1 - self.oversample_foreground_percent)) def determine_shapes(self): if self.has_prev_stage: num_seg = 2 else: num_seg = 1 k = list(self._data.keys())[0] if isfile(self._data[k][\u0026#39;data_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;): case_all_data = np.load(self._data[k][\u0026#39;data_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;, self.memmap_mode) else: case_all_data = np.load(self._data[k][\u0026#39;data_file\u0026#39;])[\u0026#39;data\u0026#39;] num_color_channels = case_all_data.shape[0] - 1 data_shape = (self.batch_size, num_color_channels, *self.patch_size) seg_shape = (self.batch_size, num_seg, *self.patch_size) return data_shape, seg_shape def generate_train_batch(self): selected_keys = np.random.choice(self.list_of_keys, self.batch_size, True, None) data = np.zeros(self.data_shape, dtype=np.float32) seg = np.zeros(self.seg_shape, dtype=np.float32) case_properties = [] for j, i in enumerate(selected_keys): # oversampling foreground will improve stability of model training, especially if many patches are empty # (Lung for example) if self.get_do_oversample(j): force_fg = True else: force_fg = False if \u0026#39;properties\u0026#39; in self._data[i].keys(): properties = self._data[i][\u0026#39;properties\u0026#39;] else: properties = load_pickle(self._data[i][\u0026#39;properties_file\u0026#39;]) case_properties.append(properties) # cases are stored as npz, but we require unpack_dataset to be run. This will decompress them into npy # which is much faster to access if isfile(self._data[i][\u0026#39;data_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;): case_all_data = np.load(self._data[i][\u0026#39;data_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;, self.memmap_mode) else: case_all_data = np.load(self._data[i][\u0026#39;data_file\u0026#39;])[\u0026#39;data\u0026#39;] # If we are doing the cascade then we will also need to load the segmentation of the previous stage and # concatenate it. Here it will be concatenates to the segmentation because the augmentations need to be # applied to it in segmentation mode. Later in the data augmentation we move it from the segmentations to # the last channel of the data if self.has_prev_stage: if isfile(self._data[i][\u0026#39;seg_from_prev_stage_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;): segs_from_previous_stage = np.load(self._data[i][\u0026#39;seg_from_prev_stage_file\u0026#39;][:-4] + \u0026#34;.npy\u0026#34;, mmap_mode=self.memmap_mode)[None] else: segs_from_previous_stage = np.load(self._data[i][\u0026#39;seg_from_prev_stage_file\u0026#39;])[\u0026#39;data\u0026#39;][None] # we theoretically support several possible previsous segmentations from which only one is sampled. But # in practice this feature was never used so it\u0026#39;s always only one segmentation seg_key = np.random.choice(segs_from_previous_stage.shape[0]) seg_from_previous_stage = segs_from_previous_stage[seg_key:seg_key + 1] assert all([i == j for i, j in zip(seg_from_previous_stage.shape[1:], case_all_data.shape[1:])]), \\ \u0026#34;seg_from_previous_stage does not match the shape of case_all_data: %svs %s\u0026#34; % \\ (str(seg_from_previous_stage.shape[1:]), str(case_all_data.shape[1:])) else: seg_from_previous_stage = None # do you trust me? You better do. Otherwise you\u0026#39;ll have to go through this mess and honestly there are # better things you could do right now # (above) documentation of the day. Nice. Even myself coming back 1 months later I have not friggin idea # what\u0026#39;s going on. I keep the above documentation just for fun but attempt to make things clearer now need_to_pad = self.need_to_pad for d in range(3): # if case_all_data.shape + need_to_pad is still \u0026lt; patch size we need to pad more! We pad on both sides # always if need_to_pad[d] + case_all_data.shape[d + 1] \u0026lt; self.patch_size[d]: need_to_pad[d] = self.patch_size[d] - case_all_data.shape[d + 1] # we can now choose the bbox from -need_to_pad // 2 to shape - patch_size + need_to_pad // 2. Here we # define what the upper and lower bound can be to then sample form them with np.random.randint shape = case_all_data.shape[1:] lb_x = - need_to_pad[0] // 2 ub_x = shape[0] + need_to_pad[0] // 2 + need_to_pad[0] % 2 - self.patch_size[0] lb_y = - need_to_pad[1] // 2 ub_y = shape[1] + need_to_pad[1] // 2 + need_to_pad[1] % 2 - self.patch_size[1] lb_z = - need_to_pad[2] // 2 ub_z = shape[2] + need_to_pad[2] // 2 + need_to_pad[2] % 2 - self.patch_size[2] # if not force_fg then we can just sample the bbox randomly from lb and ub. Else we need to make sure we get # at least one of the foreground classes in the patch if not force_fg: bbox_x_lb = np.random.randint(lb_x, ub_x + 1) bbox_y_lb = np.random.randint(lb_y, ub_y + 1) bbox_z_lb = np.random.randint(lb_z, ub_z + 1) else: # these values should have been precomputed if \u0026#39;class_locations\u0026#39; not in properties.keys(): raise RuntimeError(\u0026#34;Please rerun the preprocessing with the newest version of nnU-Net!\u0026#34;) # this saves us a np.unique. Preprocessing already did that for all cases. Neat. foreground_classes = np.array( [i for i in properties[\u0026#39;class_locations\u0026#39;].keys() if len(properties[\u0026#39;class_locations\u0026#39;][i]) != 0]) foreground_classes = foreground_classes[foreground_classes \u0026gt; 0] if len(foreground_classes) == 0: # this only happens if some image does not contain foreground voxels at all selected_class = None voxels_of_that_class = None print(\u0026#39;case does not contain any foreground classes\u0026#39;, i) else: selected_class = np.random.choice(foreground_classes) voxels_of_that_class = properties[\u0026#39;class_locations\u0026#39;][selected_class] if voxels_of_that_class is not None: selected_voxel = voxels_of_that_class[np.random.choice(len(voxels_of_that_class))] # selected voxel is center voxel. Subtract half the patch size to get lower bbox voxel. # Make sure it is within the bounds of lb and ub bbox_x_lb = max(lb_x, selected_voxel[0] - self.patch_size[0] // 2) bbox_y_lb = max(lb_y, selected_voxel[1] - self.patch_size[1] // 2) bbox_z_lb = max(lb_z, selected_voxel[2] - self.patch_size[2] // 2) else: # If the image does not contain any foreground classes, we fall back to random cropping bbox_x_lb = np.random.randint(lb_x, ub_x + 1) bbox_y_lb = np.random.randint(lb_y, ub_y + 1) bbox_z_lb = np.random.randint(lb_z, ub_z + 1) bbox_x_ub = bbox_x_lb + self.patch_size[0] bbox_y_ub = bbox_y_lb + self.patch_size[1] bbox_z_ub = bbox_z_lb + self.patch_size[2] # whoever wrote this knew what he was doing (hint: it was me). We first crop the data to the region of the # bbox that actually lies within the data. This will result in a smaller array which is then faster to pad. # valid_bbox is just the coord that lied within the data cube. It will be padded to match the patch size # later valid_bbox_x_lb = max(0, bbox_x_lb) valid_bbox_x_ub = min(shape[0], bbox_x_ub) valid_bbox_y_lb = max(0, bbox_y_lb) valid_bbox_y_ub = min(shape[1], bbox_y_ub) valid_bbox_z_lb = max(0, bbox_z_lb) valid_bbox_z_ub = min(shape[2], bbox_z_ub) # At this point you might ask yourself why we would treat seg differently from seg_from_previous_stage. # Why not just concatenate them here and forget about the if statements? Well that\u0026#39;s because segneeds to # be padded with -1 constant whereas seg_from_previous_stage needs to be padded with 0s (we could also # remove label -1 in the data augmentation but this way it is less error prone) case_all_data = np.copy(case_all_data[:, valid_bbox_x_lb:valid_bbox_x_ub, valid_bbox_y_lb:valid_bbox_y_ub, valid_bbox_z_lb:valid_bbox_z_ub]) if seg_from_previous_stage is not None: seg_from_previous_stage = seg_from_previous_stage[:, valid_bbox_x_lb:valid_bbox_x_ub, valid_bbox_y_lb:valid_bbox_y_ub, valid_bbox_z_lb:valid_bbox_z_ub] data[j] = np.pad(case_all_data[:-1], ((0, 0), (-min(0, bbox_x_lb), max(bbox_x_ub - shape[0], 0)), (-min(0, bbox_y_lb), max(bbox_y_ub - shape[1], 0)), (-min(0, bbox_z_lb), max(bbox_z_ub - shape[2], 0))), self.pad_mode, **self.pad_kwargs_data) seg[j, 0] = np.pad(case_all_data[-1:], ((0, 0), (-min(0, bbox_x_lb), max(bbox_x_ub - shape[0], 0)), (-min(0, bbox_y_lb), max(bbox_y_ub - shape[1], 0)), (-min(0, bbox_z_lb), max(bbox_z_ub - shape[2], 0))), \u0026#39;constant\u0026#39;, **{\u0026#39;constant_values\u0026#39;: -1}) if seg_from_previous_stage is not None: seg[j, 1] = np.pad(seg_from_previous_stage, ((0, 0), (-min(0, bbox_x_lb), max(bbox_x_ub - shape[0], 0)), (-min(0, bbox_y_lb), max(bbox_y_ub - shape[1], 0)), (-min(0, bbox_z_lb), max(bbox_z_ub - shape[2], 0))), \u0026#39;constant\u0026#39;, **{\u0026#39;constant_values\u0026#39;: 0}) return {\u0026#39;data\u0026#39;: data, \u0026#39;seg\u0026#39;: seg, \u0026#39;properties\u0026#39;: case_properties, \u0026#39;keys\u0026#39;: selected_keys}    学习率初始是0.01 进行了梯度剪裁 python torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12) 网络当中还是随机裁剪了，进入网络path的大小为(2,1,64,192,192)\n图像为原尺寸大小，但是label被缩小成了每一层的大小。注意，这里的label包含了所有的标签，同时包含了胰腺和肿瘤 \n网络也会同时输出五层的结果，在loss中通过权重进行分配，权重分别是体素的比\n  \\[\\begin{aligned}64 \u0026\\times \u0026192 \u0026\\times \u0026192 \u0026(64) \u0026: \\\\32 \u0026\\times \u002696 \u0026\\times \u002696 \u0026(32) \u0026: \\\\16 \u0026\\times \u002648 \u0026\\times \u002648 \u0026(16) \u0026: \\\\8 \u0026\\times \u002624 \u0026\\times \u002624 \u0026(8) \u0026: \\\\4 \u0026\\times \u002612 \u0026\\times \u002612 \u0026(1) \u0026:\\end{aligned}\\]\n更改（windows）  maybe_mkdir_p $\\rightarrow$ os.makedirs  ","description":"","id":18,"section":"posts","tags":["代码复现","医学图像","分割","nnunet"],"title":"nnunet学习笔记","uri":"https://miaomiaoyoung.github.io/en/posts/nnunet/"},{"content":"Hugo 更新日志 https://docs.stack.jimmycai.com/v/zh-cn/configuration\n  增加了Katex引擎\nhttps://eankeen.github.io/blog/posts/render-latex-with-katex-in-hugo-blog/\n新增了./layouts/partials/katex.html\n改动了./layouts/partials/fotter/include.html\n  增加了阅读统计，字数，分钟数等\nhttps://blog.csdn.net/weixin_45433910/article/details/108177710\n更改了.\\myblog\\themes\\hugo-theme-stack\\layouts\\partials\\article\\components\\header.html\n更改了.\\myblog\\themes\\hugo-theme-stack\\layouts\\partials\\article\\components\\details.html\n  ","description":"","id":19,"section":"posts","tags":null,"title":"My First Post","uri":"https://miaomiaoyoung.github.io/en/posts/my-first-post/"},{"content":"Sample images from Pixabay\n","description":"cartoon gallery","id":20,"section":"gallery","tags":null,"title":"Cartoon","uri":"https://miaomiaoyoung.github.io/en/gallery/cartoon/"},{"content":"我是这样的一个人，常常陷入到自己的一些情绪里面，\n然后一点点开始表演这种情绪，希望这种表演被发现，\n但往往都是自己对自己的审讯，久了成为精神上的怪病\n他逐渐变成具体的愤怒和对峙中的口无遮拦\n他让人在表述上违背本意\n逐渐把进入愤怒和出离平静都变得让人捉摸不透\n它带来的外化部分是情绪和行为处在长期的一个错位中\n同时的话在矛盾里面有透着很多的温顺\n温顺之上又是淤积的一些暴怒，暴怒之后常常陷入一些卑贱\n憎恶自怜自爱但是自己常常又不自持\n对时间的慌张很强烈，自卑里挖掘出一种傲慢\n随即又被懦弱重新埋到了悲观里面\n但是我掩盖了这一部分的缺陷\n只在一些阴暗的夜的里面让他们盘根结果\n太阳一出来的时候就把他们全部铲断\n我以虚伪的快乐示人\n把柔和跟感动看得及其重要\n因为在上一个夜晚我觉得自己太丑恶了\n我对宇宙和太空无比的迷恋\n常常用这类电影和书籍消化自己的情绪\n去掉生命的意义把自己打回物质\n我的左手和右手甚至都不是来自同一颗恒星\n如果是这样的话\n我何必因为自己生命力关于欲望的\n那部分不如意来折磨自己\n于是我感到变小变轻，到达豁达和平静\n我存在的意义也不是生命可达到的那种意义\n而是宇宙里物质的意义\n","description":"Hugo, the world’s fastest framework for building websites","id":26,"section":"","tags":null,"title":"About","uri":"https://miaomiaoyoung.github.io/en/about/"}]